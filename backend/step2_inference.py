# %%
import torch
import wandb

from utils import *

# Parameters
load_model='/models/test_model'
train_args='outdir_step1/train_args.yml'
max_seq_len=-1
freeze_predecoder=False
batch_size=32
epochs=1
save_dir='outdir_step2'

#
hyperparameter_defaults = load_config(eval=True, load_model_dir=load_model, train_args=train_args)

# update configs
task_info = hyperparameter_defaults['task_info']
preprocess_config = hyperparameter_defaults['preprocess']
model_params = hyperparameter_defaults['model_parameters']
task_configs = hyperparameter_defaults['task_configs']
wandb_config = hyperparameter_defaults['wandb_configs']

model_params['load_model'] = load_model
model_params['epochs'] = epochs
model_params['batch_size'] = batch_size
model_params['freeze_predecoder'] = freeze_predecoder
task_configs['max_seq_len'] = task_configs['max_seq_len'] if max_seq_len == -1 else max_seq_len
wandb_config['mode'] = 'offline'
wandb_config['project'] = 'inference'
wandb_config['name'] = ''

# Create WandB instance and enable cloud-syncing
wandb_instance = ProtocolWandB(hyperparameter_defaults)
run = wandb_instance.create_wandb_project()
config = wandb.config  # easy-dict access

# START TIME POINT
START_POINT = time.time()

# create local save directory for fine-tuning and save updated config file
set_seed(model_params['seed'])
dataset_name = task_info['raw_dataset_name']
save_dir = Path(save_dir)
save_dir.mkdir(parents=True, exist_ok=True)
logger = scg.logger
scg.utils.add_file_handler(logger, save_dir / "run.log")
logger.info(f"Current evaluation progress is saved to -> {save_dir}")
logger.info(f"Working directory is initialized successfully ...")

# CONSTANTS
pad_token = config.task_configs['pad_token']
special_tokens = [pad_token, config.task_configs['cls_token'], config.task_configs['eoc_token']]
mask_value = config.task_configs['mask_value']
pad_value = config.task_configs['pad_value']
append_cls = config.task_configs['append_cls']
include_zero_gene = config.task_configs['include_zero_gene']
max_seq_len = config.task_configs['max_seq_len']
n_input_bins = config.task_configs['n_input_bins']
_cell_type_col = preprocess_config['_FIXED_CELL_TYPE_COL']
_cell_type_id = preprocess_config['_FIXED_CELL_TYPE_ID_COL']

## Task configurations
CLS = config.task_configs['CLS']
input_emb_style = config.task_configs['input_emb_style']
cell_emb_style = config.task_configs['cell_emb_style']
ecs_threshold = config.task_configs['ecs_threshold']

## Model Parameters
lr = config.model_parameters['lr']
batch_size = config.model_parameters['batch_size']
epochs = config.model_parameters['epochs']
schedule_interval = config.model_parameters['schedule_interval']
fast_transformer = config.model_parameters['use_flash_attn']
fast_transformer_backend = config.model_parameters['fast_transformer_backend']
embsize = config.model_parameters['embsize']
d_hid = config.model_parameters['d_hid']
nlayers = config.model_parameters['nlayers']
nhead = config.model_parameters['nheads']
nlayers_cls = config.model_parameters['nlayers_cls']
dropout = config.model_parameters['dropout']
log_interval = config.model_parameters['log_interval']
save_eval_interval = config.model_parameters['save_eval_interval']

# Read input annData
adata = sc.read_h5ad(config.task_info['input_dataset_directory'], backed='r')
genes = adata.var[config.preprocess['_FIXED_GENE_COL']].tolist()

# Load pre-trained model weights
logger.info(f'Start loading pre-trained model and weights ...')
model_dir = Path(model_params['load_model'])
model_file = model_dir / "best_model.pt"  # Change the model file name if different
model_config_file = model_dir / "args.json"
vocab_file = model_dir / "vocab.json"
vocab = get_vocab(vocab_file, special_tokens)
vocab.set_default_index(vocab["<pad>"])
gene_ids = np.array(vocab(genes), dtype=int)
logger.info(
    f"Resume model from {model_file}, the model args will be override by the "
    f"config {model_config_file}."
)

# Create data collator and loaders
data_collator = DataCollator(
    do_padding=True,
    pad_token_id="<pad>",
    do_mlm=False,
    do_binning=True,
    mask_value=mask_value,
    pad_value=pad_value,
    max_length=max_seq_len,
    data_style="cls",
    filtered_gene_list=gene_ids,
    vocab=vocab,
    append_cls=append_cls,
    include_zero_gene=include_zero_gene
)

test_dataset = SeqDataset(adata, cell_type_id_col=_cell_type_id)
test_sampler = (SequentialSampler(test_dataset))
test_loader = DataLoader(
    test_dataset,
    batch_size=batch_size,
    sampler=test_sampler,
    collate_fn=data_collator,
    drop_last=False,
    num_workers=0,  # !!! Enable data stream, ONLY 1 process
    pin_memory=True
)

# Create model instance
logger.info(f'Create new model instance ...')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
ntokens = len(vocab)
id2type_json_path = Path(load_model, config.task_info['id2type_json'].split('/')[-1]) if load_model else \
config.task_info['id2type_json']
with open(id2type_json_path, 'r') as f:
    ref_id2type = json.load(f)
unique_ref_cell_types = list(ref_id2type.values())
ref_total_types = len(unique_ref_cell_types)
model = TransformerModel(
    ntokens,
    embsize,
    nhead,
    d_hid,
    nlayers,
    nlayers_cls=nlayers_cls,
    n_cls=ref_total_types if CLS else 1,
    vocab=vocab,
    dropout=dropout,
    pad_token=pad_token,
    pad_value=pad_value,
    do_mvc=config.task_configs['MVC'],
    do_dab=config.task_configs['DAB'],
    use_batch_labels=config.model_parameters['use_batch_labels'],
    domain_spec_batchnorm=config.model_parameters['DSBN'],
    input_emb_style=input_emb_style,
    n_input_bins=n_input_bins,
    cell_emb_style=cell_emb_style,
    ecs_threshold=ecs_threshold,
    use_fast_transformer=fast_transformer,
    fast_transformer_backend=fast_transformer_backend,
    pre_norm=config.model_parameters['pre_norm'],
)
if config.model_parameters['load_model'] is not None:
    try:
        model.load_state_dict(torch.load(model_file))
        logger.info(f"Loading all model params from {model_file}")
    except:
        # only load params that are in the model and match the size
        model_dict = model.state_dict()
        pretrained_dict = torch.load(model_file, map_location=device)
        pretrained_dict = {
            k: v
            for k, v in pretrained_dict.items()
            if k in model_dict and v.shape == model_dict[k].shape
        }
        for k, v in pretrained_dict.items():
            logger.info(f"Loading params {k} with shape {v.shape}")
        model_dict.update(pretrained_dict)
        model.load_state_dict(model_dict)

pre_freeze_param_count = sum(
    dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())

# Freeze all pre-decoder weights
for name, para in model.named_parameters():
    if config.model_parameters['freeze_predecoder'] and "encoder" in name and "transformer_encoder" not in name:
        # if config.freeze and "encoder" in name:
        print(f"freezing weights for: {name}")
        para.requires_grad = False

post_freeze_param_count = sum(
    dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())

logger.info(f"Total Pre freeze Params {(pre_freeze_param_count)}")
logger.info(f"Total Post freeze Params {(post_freeze_param_count)}")

model.to(device)
is_parallel = torch.cuda.device_count() > 1
if is_parallel:
    model = nn.DataParallel(model)

wandb.watch(model)

# Define loss, optimizer, scaler
logger.info(f'Define loss metrics and optimizer ...')
criterion_cls = nn.CrossEntropyLoss()

logger.info(f'Start annotating ... ')
labels = adata.obs[_cell_type_id].tolist() if adata.obs.get(_cell_type_id, None) is not None else None
(
    predictions,
    results,
    precision_dict,
    wrong_predictions,
    confidences
) = test(
    model,
    loader=test_loader,
    adata=adata,
    true_cell_type_ids=labels,
    ref_id2type=ref_id2type,
    pad_vocab=vocab[pad_token],
    criterion_cls=criterion_cls,
    config=config,
    epoch=epochs,
    logger=logger,
    device=device
)

# END TIME POINT
INFERENCE_END_POINT = time.time()
inference_time = round(INFERENCE_END_POINT - START_POINT, 2)
wandb.log(
    {
        "Inference Time": inference_time
    }
)

logger.info(f'*** Inference was finished in: {inference_time} seconds ***')
adata.obs['predictions'] = [ref_id2type[str(pred)] for pred in predictions]
subset_obs = adata.obs[['predictions']].copy()
subset_obs['confidence'] = confidences.max(axis=1)

subset_obs = subset_obs.reset_index()
subset_obs.to_csv(save_dir / 'predictions.csv', index=False)
print('>' * 30)
logger.info(f'Results are saved to directory ==> {save_dir}')
logger.info(f'Inference was completed ! Well done =) ')
print('<' * 30)
run.finish()
wandb.finish()
gc.collect()
